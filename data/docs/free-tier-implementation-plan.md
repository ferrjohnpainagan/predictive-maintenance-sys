# Data Pipeline Free Tier Implementation Plan - Predictive Maintenance System

## Project Overview

Implementation plan for the data pipeline using AWS Free Tier services and local tools. This approach focuses on cost-effective data processing, storage, and quality management while staying within free tier limits.

## Technology Stack (Free Tier)

- **ETL Processing**: Python scripts with Pandas/NumPy (optimized for t2.micro)
- **Data Storage**: Supabase PostgreSQL (Free tier: 500MB, 2 projects)
- **File Storage**: S3 (5GB free) for ML data and backups
- **Stream Processing**: Local data simulation (no Kinesis for free tier)
- **Data Quality**: Great Expectations (free tool)
- **Orchestration**: GitHub Actions scheduling + Local scripts
- **Monitoring**: CloudWatch integration (free tier)

## Free Tier Considerations

- **Supabase**: 500MB database limit (sufficient for C-MAPSS dataset)
- **S3**: 5GB storage, 20K GET requests, 2K PUT requests
- **EC2**: t2.micro memory constraints (1GB RAM)
- **No expensive AWS services**: Avoid MWAA, Kinesis, Step Functions
- **Cost**: $0/month (within free tier limits)

---

## Phase 1: Data Architecture Setup (Free Tier)

**Duration**: 3-4 days  
**Priority**: Critical

### Objectives

- Design data flow architecture optimized for free tier
- Set up storage layers using free services
- Configure data governance framework

### Tasks

1. **Free Tier Data Architecture**

   ```
   Data Flow (Free Tier):

   C-MAPSS Dataset → Python Scripts → Supabase PostgreSQL
                    ↓
   Real-time Simulation → Local Processing → Supabase + S3
                    ↓
   Data Quality Checks → Great Expectations → CloudWatch Logs
                    ↓
   Backup & Archival → S3 (with lifecycle policies)
   ```

2. **Supabase Database Schema (Free Tier Optimized)**

   ```sql
   -- engines table (optimized for free tier)
   CREATE TABLE engines (
     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
     unit_number VARCHAR(50) UNIQUE NOT NULL,
     model VARCHAR(100),
     manufactured_date DATE,
     first_operation_date DATE,
     status VARCHAR(20) DEFAULT 'operational',
     created_at TIMESTAMP DEFAULT NOW(),
     updated_at TIMESTAMP DEFAULT NOW()
   );

   -- sensor_data table (partitioned for performance)
   CREATE TABLE sensor_data (
     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
     engine_id UUID REFERENCES engines(id),
     cycle INTEGER NOT NULL,
     timestamp TIMESTAMP NOT NULL,

     -- Operational Settings
     altitude REAL,           -- setting_1 (ft)
     mach_number REAL,        -- setting_2
     tra REAL,                -- setting_3 (Throttle Resolver Angle %)

     -- Sensor Readings (14 key sensors - optimized selection)
     s2 REAL,   -- T24 - Total temperature at LPC outlet (°R)
     s3 REAL,   -- T30 - Total temperature at HPC outlet (°R)
     s4 REAL,   -- T50 - Total temperature at LPT outlet (°R)
     s7 REAL,   -- P30 - Total pressure at HPC outlet (psia)
     s8 REAL,   -- Nf - Physical fan speed (rpm)
     s9 REAL,   -- Nc - Physical core speed (rpm)
     s11 REAL,  -- Ps30 - Static pressure at HPC outlet (psia)
     s12 REAL,  -- phi - Ratio of fuel flow to Ps30 (pps/psia)
     s13 REAL,  -- NRf - Corrected fan speed (rpm)
     s14 REAL,  -- NRc - Corrected core speed (rpm)
     s15 REAL,  -- BPR - Bypass Ratio
     s17 REAL,  -- htBleed - HPT coolant bleed (lbm/s)
     s20 REAL,  -- W31 - LPT coolant bleed (lbm/s)
     s21 REAL,  -- W32 - HPT coolant bleed (lbm/s)

     created_at TIMESTAMP DEFAULT NOW()
   );

   -- Create indexes for performance (free tier optimization)
   CREATE INDEX idx_sensor_data_engine_id ON sensor_data(engine_id);
   CREATE INDEX idx_sensor_data_timestamp ON sensor_data(timestamp);
   CREATE INDEX idx_sensor_data_cycle ON sensor_data(cycle);

   -- rul_predictions table
   CREATE TABLE rul_predictions (
     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
     engine_id UUID REFERENCES engines(id),
     predicted_rul INTEGER NOT NULL,
     confidence REAL,
     model_version VARCHAR(50),
     prediction_timestamp TIMESTAMP DEFAULT NOW(),
     input_data_range TSTZRANGE,
     created_at TIMESTAMP DEFAULT NOW()
   );

   -- data_quality_metrics table
   CREATE TABLE data_quality_metrics (
     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
     table_name VARCHAR(100),
     metric_name VARCHAR(100),
     metric_value REAL,
     threshold_min REAL,
     threshold_max REAL,
     status VARCHAR(20),
     check_timestamp TIMESTAMP DEFAULT NOW()
   );

   -- data_processing_logs table (for monitoring)
   CREATE TABLE data_processing_logs (
     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
     process_name VARCHAR(100),
     status VARCHAR(20),
     records_processed INTEGER,
     processing_time_ms INTEGER,
     error_message TEXT,
     created_at TIMESTAMP DEFAULT NOW()
   );
   ```

3. **S3 Data Organization (Free Tier Optimized)**

   ```
   s3://predictive-maintenance-free-tier-ml-data/
   ├── raw/                    # Original C-MAPSS files
   │   ├── train_FD001.txt     # ~2.5MB
   │   ├── test_FD001.txt      # ~2.5MB
   │   └── RUL_FD001.txt       # ~0.1MB
   ├── processed/              # Cleaned and transformed data
   │   ├── year=2024/
   │   │   ├── month=01/
   │   │   │   ├── day=01/
   │   │   │   │   └── sensor_data.parquet
   │   └── train-test-split/
   │       ├── X_train.parquet
   │       ├── y_train.parquet
   │       ├── X_test.parquet
   │       └── y_test.parquet
   ├── quality/                # Data quality reports
   │   ├── expectations/
   │   └── reports/
   └── backups/               # Database backups (daily)
       └── daily/

   Total estimated size: ~10MB (well within 5GB free tier)
   ```

### Deliverables

- ✅ Database schema created and deployed (free tier optimized)
- ✅ S3 bucket structure organized (within 5GB limit)
- ✅ Data governance framework defined
- ✅ Initial data catalog

---

## Phase 2: C-MAPSS Dataset Ingestion (Free Tier)

**Duration**: 3-4 days  
**Priority**: Critical

### Objectives

- Import C-MAPSS dataset using free tools
- Transform and load into Supabase (within 500MB limit)
- Validate data integrity and quality

### Tasks

1. **Free Tier Data Ingestion Pipeline**

   - Download C-MAPSS dataset from NASA (public domain)
   - Parse and clean data files with Python scripts
   - Calculate RUL (Remaining Useful Life) labels
   - Add realistic timestamps for operational cycles
   - Batch load into Supabase with progress tracking

2. **Free Tier Data Validation Pipeline**

   - Implement Great Expectations for data quality
   - Create validation rules for sensor ranges
   - Validate data types and constraints
   - Generate quality reports and metrics

3. **Free Tier Orchestration (GitHub Actions)**
   - Schedule daily data quality checks
   - Weekly database backups
   - Monthly data cleanup procedures

### Deliverables

- ✅ C-MAPSS data ingested into Supabase (within 500MB limit)
- ✅ Data validation pipeline operational
- ✅ Data quality reports generated
- ✅ Database queries working correctly
- ✅ Free tier constraints respected

---

## Phase 3: Real-time Data Simulation (Free Tier)

**Duration**: 4-5 days  
**Priority**: High

### Objectives

- Create real-time sensor data simulation (no Kinesis)
- Implement continuous data ingestion using free tools
- Set up data streaming for ML training

### Tasks

1. **Free Tier Sensor Data Simulator**

   - Generate realistic sensor readings based on C-MAPSS patterns
   - Simulate engine degradation over time
   - Create operational scenarios (takeoff, cruise, landing)
   - Generate data at configurable intervals

2. **Free Tier Data Ingestion Automation**
   - Local data streaming (no Kinesis for free tier)
   - Batch processing for efficiency
   - Data quality monitoring in real-time
   - Alerting for data anomalies

### Deliverables

- ✅ Real-time data simulation operational (no Kinesis)
- ✅ Continuous data ingestion pipeline
- ✅ Data quality monitoring active
- ✅ Automated backup procedures
- ✅ Free tier constraints respected

---

## Success Criteria

### **Technical KPIs**

- [ ] C-MAPSS dataset successfully ingested (>100K records)
- [ ] Data validation pipeline operational
- [ ] Real-time simulation generating data
- [ ] Database performance within acceptable limits
- [ ] Data quality metrics >95% pass rate
- [ ] **Free tier limits respected** (Supabase <500MB, S3 <5GB)

### **Operational KPIs**

- [ ] Data pipeline runs without manual intervention
- [ ] Data quality alerts configured and working
- [ ] Backup procedures tested and verified
- [ ] Documentation complete and accessible
- [ ] **Cost maintained at $0/month**

## Free Tier Optimizations

### **Memory Management**

- **Batch Processing**: Small batches (500 records) for t2.micro
- **Data Types**: Optimized numpy dtypes (float32, int32)
- **Garbage Collection**: Explicit cleanup after large operations

### **Storage Optimization**

- **S3 Lifecycle**: Automatic cleanup of old data
- **Parquet Format**: Efficient compression for ML data
- **Partitioning**: Smart data organization for queries

### **Processing Efficiency**

- **Local Scripts**: No expensive AWS services
- **GitHub Actions**: Free orchestration and scheduling
- **Great Expectations**: Free data quality tool

### **Cost Control**

- **No MWAA**: Use local scripts instead
- **No Kinesis**: Use local simulation instead
- **No Step Functions**: Use GitHub Actions instead
- **No DataDog**: Use CloudWatch instead

## Next Steps After Completion

1. **MLOps Pipeline Setup** (Phase 8)
2. **ML Service Development** (Phase 9)
3. **API Gateway Integration** (Phase 10)

## Resource Links

- **FJ-13 Ticket**: [Phase 7: Data Pipeline Implementation](../tickets/FJ-13.md)
- **Infrastructure**: [infrastructure/docs/free-tier-implementation-plan.md](../infrastructure/docs/free-tier-implementation-plan.md)
- **Monitoring**: [monitoring/docs/free-tier-implementation-plan.md](../monitoring/docs/free-tier-implementation-plan.md)

---

**Status**: 🔄 **READY TO START**  
**Priority**: **High**  
**Estimated Duration**: **2 weeks**  
**Dependencies**: ✅ **Infrastructure Complete**  
**Free Tier Compliance**: ✅ **100% Compliant**
