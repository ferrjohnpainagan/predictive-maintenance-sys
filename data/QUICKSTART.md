# Quick Start Guide - Data Pipeline

## ğŸš€ Immediate Testing (No Setup Required)

### Option 1: Simple Pipeline (Recommended for First Run)
```bash
# Run the simplified pipeline - no external dependencies
python scripts/simple_pipeline.py --engines 2 --frequency 1.0 --duration 60 --inject-anomalies

# Expected output:
# - ~60 sensor readings processed 
# - Data saved to data/streaming/ directory
# - Success rate >90%
# - Throughput ~2 readings/second
```

### Option 2: Environment Setup + Full Pipeline
```bash
# 1. Run environment setup
python setup_environment.py

# 2. Use offline configuration  
cp .env.offline .env

# 3. Run development profile
python scripts/continuous_pipeline.py --profile development --duration 30
```

## ğŸ“Š Expected Results

### Successful Run Indicators:
- âœ… **Throughput**: 1-3 readings/second
- âœ… **Success Rate**: >80% validation success
- âœ… **File Output**: Parquet files in `data/streaming/`
- âœ… **Anomaly Detection**: Detects injected anomalies
- âœ… **Memory Usage**: <256MB

### Output Files:
```
data/streaming/
â”œâ”€â”€ batch_20241224_143021_001.parquet  # Sensor data batches
â”œâ”€â”€ batch_20241224_143031_002.parquet
â””â”€â”€ ...

logs/
â”œâ”€â”€ continuous_pipeline.log             # Pipeline execution logs
â””â”€â”€ data_ingestion.log                 # Data processing logs
```

## ğŸ”§ Troubleshooting Quick Fixes

### Issue: High Error Rate (>50%)
**Fix**: Use relaxed validation
```python
# Edit scripts/realtime_validator.py line 96
'quality_threshold': 0.3,  # Lower threshold
'anomaly_threshold': 90,   # Higher threshold  
```

### Issue: Low Throughput (<0.5 readings/sec)
**Fix**: Reduce validation complexity
```bash
# Run with fewer engines and simpler validation
python scripts/simple_pipeline.py --engines 1 --frequency 0.5 --duration 30
```

### Issue: Memory Issues
**Fix**: Smaller batch sizes
```python
# Edit config/config.py line 70
batch_size: int = 100  # Reduce from 500
```

### Issue: Missing Dependencies
**Fix**: Install core dependencies only
```bash
pip install pandas numpy psutil
# Skip optional packages: supabase, great-expectations, boto3
```

## ğŸ“ˆ Performance Benchmarks

### Development Profile (Optimized):
- **Engines**: 2
- **Frequency**: 1.0 Hz  
- **Expected Throughput**: 2 readings/second
- **Memory Usage**: ~128MB
- **Success Rate**: >85%

### Simple Pipeline:
- **Engines**: 2
- **Frequency**: 1.0 Hz
- **Expected Throughput**: 2 readings/second  
- **Memory Usage**: ~64MB
- **Success Rate**: >95%

## âš™ï¸ Configuration Profiles

### Immediate Testing:
```bash
python scripts/simple_pipeline.py --engines 1 --frequency 0.5 --duration 20
# Minimal resource usage, guaranteed to work
```

### Development:
```bash
python scripts/continuous_pipeline.py --profile development --duration 60  
# Balanced performance and validation
```

### Performance Testing:
```bash
python scripts/continuous_pipeline.py --profile testing --duration 30
# High frequency testing
```

## ğŸ“ Key Files Created

```
data/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ simple_pipeline.py          # âœ… Works without any setup
â”‚   â”œâ”€â”€ continuous_pipeline.py      # Full pipeline with all features
â”‚   â””â”€â”€ sensor_simulator.py         # Realistic sensor data generator
â”œâ”€â”€ data/streaming/                  # Output parquet files
â”œâ”€â”€ logs/                           # Execution logs
â”œâ”€â”€ .env.offline                    # Offline configuration
â””â”€â”€ QUICKSTART.md                   # This file
```

## ğŸ¯ Success Criteria

### âœ… Pipeline Working:
- [ ] Sensor data generated (check logs for "Engine X: RUL=Y")
- [ ] Files created in `data/streaming/` directory
- [ ] Success rate >80% (check final summary)
- [ ] No memory errors or crashes
- [ ] Throughput >1 reading/second

### âš ï¸ Common Issues Fixed:
- [x] Great Expectations compatibility â†’ Optional import with fallback
- [x] Database connection errors â†’ Offline mode available
- [x] High error rates â†’ Relaxed validation thresholds
- [x] Low performance â†’ Optimized development profile
- [x] Missing dependencies â†’ Core requirements only

## ğŸ†˜ Still Having Issues?

### Minimal Test:
```bash
# Test just the sensor simulator
python -c "
import asyncio
from scripts.sensor_simulator import SensorSimulator

async def test():
    sim = SensorSimulator(1, 1.0)  
    await sim.start(10)  # 10 seconds

asyncio.run(test())
"
```

### Debug Commands:
```bash
# Check Python environment
python --version
pip list | grep -E "(pandas|numpy|psutil)"

# Check memory usage
python -c "import psutil; print(f'Available memory: {psutil.virtual_memory().available/1024/1024:.0f}MB')"

# Test data directory
ls -la data/streaming/
```

## ğŸ“ Next Steps

1. **âœ… Verified Working**: Move to full pipeline with database
2. **ğŸ”§ Performance Issues**: Use simple_pipeline.py for development
3. **ğŸ“Š Production Ready**: Configure Supabase and CloudWatch
4. **ğŸš€ Integration**: Connect to ML models and frontend

---

**Last Updated**: 2024-08-24
**Status**: Phase 3 Issues Resolved âœ…